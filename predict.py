import warnings
warnings.filterwarnings('ignore')
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from pytorchvideo.models.hub import x3d_xs
from tqdm import tqdm
import torch.nn.functional as F
from torch.utils.data import Dataset
import pandas as pd
from torchvision.io import read_video
from torchvision import transforms as T
import cv2
import numpy as np
from torchvision import transforms
import random
from PIL import Image
from torch.utils.data import ConcatDataset
from torch.utils.data import Subset
import copy
from PIL import Image, ImageFilter
import json
import warnings
warnings.filterwarnings('ignore')

def read_video_cv2(path, max_frames=240, sample_frames=120):
    # é–‹å•Ÿå½±ç‰‡æª”æ¡ˆ
    cap = cv2.VideoCapture(path)
    frames = []

    # é€å¹€è®€å–å½±ç‰‡ç›´åˆ°çµæŸ
    while True:
        ret, frame = cap.read()
        if not ret:
            break  # å¦‚æœè®€ä¸åˆ°ï¼ˆå½±ç‰‡çµæŸï¼‰ï¼Œå°±è·³å‡º
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # BGRè½‰RGB
        frames.append(frame)

    # é‡‹æ”¾å½±ç‰‡è³‡æº
    cap.release()

    # è‹¥è®€ä¸åˆ°ä»»ä½•å¹€ï¼Œä¸Ÿå‡ºéŒ¯èª¤
    total_frames = len(frames)
    if total_frames == 0:
        raise RuntimeError(f"Cannot read video {path}")

    # è‹¥å½±ç‰‡å¹€æ•¸ä¸è¶³ max_framesï¼Œé‡è¤‡æœ€å¾Œä¸€å¹€ä¾†è£œè¶³é•·åº¦
    while len(frames) < max_frames:
        frames.append(frames[-1].copy())

    # é™åˆ¶æœ€å¤šåªå– max_frames å¹€
    frames = frames[:max_frames]

    # ç­‰è·åœ°é¸å‡º sample_frames å¹€ç´¢å¼•ä½ç½®
    indices = np.linspace(0, max_frames - 1, sample_frames).astype(int)

    # æ ¹æ“šç´¢å¼•å–å‡ºå°æ‡‰çš„å¹€
    sampled_frames = [frames[i] for i in indices]

    # å°‡å¹€åˆ—è¡¨è½‰æˆ NumPy é™£åˆ— (T, H, W, C)
    video_np = np.stack(sampled_frames, axis=0)

    # è½‰æˆ PyTorch Tensor ä¸¦é‡æ–°æ’åˆ—ç¶­åº¦ç‚º (C, T, H, W)
    video_t = torch.from_numpy(video_np).permute(3, 0, 1, 2)

    # å›å‚³å½±ç‰‡å¼µé‡
    return video_t

# è³‡æ–™æ¨¡å‹
class Normalize(torch.nn.Module):
    def __init__(self, mean, std):
        super().__init__()  # ç¹¼æ‰¿è‡ª nn.Module çš„åˆå§‹åŒ–
        # å°‡ mean è½‰æˆå¼µé‡ï¼Œä¸¦ reshape æˆ (C, 1, 1, 1)ï¼Œæ–¹ä¾¿èˆ‡ (C, T, H, W) çš„å½±åƒé€²è¡Œ broadcast
        self.mean = torch.tensor(mean).view(-1, 1, 1, 1)
        # å°‡ std è½‰æˆå¼µé‡ï¼Œä¸¦ reshape æˆ (C, 1, 1, 1)ï¼Œæ–¹ä¾¿èˆ‡å½±åƒè³‡æ–™é€²è¡Œé™¤æ³• broadcast
        self.std = torch.tensor(std).view(-1, 1, 1, 1)

    def forward(self, x):
        # å°è¼¸å…¥å¼µé‡ x åšæ¨™æº–åŒ–ï¼šæ¯å€‹ channel æ¸›å»å¹³å‡å€¼ã€é™¤ä»¥æ¨™æº–å·®
        return (x - self.mean) / self.std


# ğŸ”§ å®‰å…¨è³‡æ–™å¢å¼·
class SafeVideoAugmentation:
    def __init__(self, resize=(224, 224), apply_blur_prob=0.3, apply_brightness_prob=0.3):
        # è¨­å®šæ¯å¹€å½±åƒçš„ resize å¤§å°
        self.resize = resize

        # å®šç¾©å°‡ PIL å½±åƒè½‰æˆ tensor çš„è½‰æ›å™¨ (0~1 ç¯„åœ)
        self.to_tensor = transforms.ToTensor()

        # å®šç¾©æ‡‰ç”¨æ¨¡ç³Šçš„æ©Ÿç‡
        self.apply_blur_prob = apply_blur_prob

        # å®šç¾©æ‡‰ç”¨äº®åº¦èª¿æ•´çš„æ©Ÿç‡
        self.apply_brightness_prob = apply_brightness_prob

        # å®šç¾©å½±ç‰‡çš„æ¨™æº–åŒ–æ–¹å¼ (ä½¿ç”¨ ImageNet æ¨™æº–)
        self.normalize = Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])

    def __call__(self, frames):
        augmented = []  # ç”¨ä¾†å„²å­˜å¢å¼·å¾Œçš„æ¯ä¸€å¹€

        # éš¨æ©Ÿæ±ºå®šæ˜¯å¦å°æ•´å€‹å½±ç‰‡æ‡‰ç”¨æ¨¡ç³Š
        apply_blur = random.random() < self.apply_blur_prob

        # éš¨æ©Ÿæ±ºå®šæ˜¯å¦å°æ•´å€‹å½±ç‰‡æ‡‰ç”¨äº®åº¦èª¿æ•´
        apply_brightness = random.random() < self.apply_brightness_prob

        # éš¨æ©Ÿç”¢ç”Ÿä¸€å€‹äº®åº¦èª¿æ•´ä¿‚æ•¸ (0.8~1.2 ä¹‹é–“)
        brightness_factor = random.uniform(0.8, 1.2)

        # éæ­·æ¯ä¸€å¹€åšå¢å¼·è™•ç†
        for frame in frames:
            # å°‡å½±åƒ resize æˆå›ºå®šå¤§å°
            frame = cv2.resize(frame, self.resize)

            # å°‡ NumPy é™£åˆ—è½‰ç‚º PIL å½±åƒï¼Œæ–¹ä¾¿å¾ŒçºŒä½¿ç”¨ PIL çš„å½±åƒå¢å¼·æ–¹æ³•
            pil_frame = Image.fromarray(frame)

            # è‹¥è¦å¥—ç”¨æ¨¡ç³Šï¼Œå‰‡åŠ ä¸Š GaussianBlur
            if apply_blur:
                pil_frame = pil_frame.filter(ImageFilter.GaussianBlur(radius=1.5))  # radius æ§åˆ¶æ¨¡ç³Šå¼·åº¦

            # è‹¥è¦å¥—ç”¨äº®åº¦èª¿æ•´ï¼Œå‰‡é€²è¡Œäº®åº¦å¢å¼·
            if apply_brightness:
                pil_frame = transforms.functional.adjust_brightness(pil_frame, brightness_factor)

            # âœ… å°‡å½©è‰²å½±åƒè½‰ç‚ºç°éšï¼ˆå–®é€šé“ï¼‰å†è½‰å› RGBï¼ˆä¸‰é€šé“ï¼‰
            gray_frame = pil_frame.convert("L")         # è½‰ç°éš
            gray_frame = gray_frame.convert("RGB")      # å†è½‰å› RGBï¼ˆR=G=Bï¼‰

            # å°‡ PIL å½±åƒè½‰æˆ tensorï¼Œå½¢ç‹€ç‚º (C, H, W)ï¼Œå€¼åœ¨ 0~1
            tensor_frame = self.to_tensor(gray_frame)

            # åŠ å…¥å¢å¼·å¾Œçš„å¹€
            augmented.append(tensor_frame)

        # å°‡æ‰€æœ‰å¹€å †ç–Šèµ·ä¾†æˆ (T, C, H, W)
        augmented_tensor = torch.stack(augmented)

        # å°‡ç¶­åº¦è½‰æ›æˆ (C, T, H, W)ï¼Œç¬¦åˆå½±ç‰‡æ¨¡å‹è¼¸å…¥æ ¼å¼
        augmented_tensor = augmented_tensor.permute(1, 0, 2, 3)

        # å°æ•´æ®µå½±ç‰‡å¼µé‡åšæ¨™æº–åŒ–è™•ç†
        augmented_tensor = self.normalize(augmented_tensor)

        # å›å‚³å¢å¼·èˆ‡æ¨™æº–åŒ–å¾Œçš„å½±ç‰‡å¼µé‡
        return augmented_tensor #æˆ‘å¸Œæœ›å°‡å½±ç‰‡è½‰ç°éšä½†æ˜¯ä¿ç•™é€šé“æ•¸

# é€™è£¡å°‡æ¨¡å‹model_pathçš„æ¬Šé‡è¼‰å…¥
model_path = './model/epoch_2_valacc_0.5646.pt'
model = x3d_xs(pretrained=True)  # è¼‰å…¥é è¨“ç·´çš„X3D XSæ¨¡å‹
model.blocks[-1].proj = nn.Linear(model.blocks[-1].proj.in_features, 2)  # ä¿®æ”¹æœ€å¾Œåˆ†é¡å±¤ç‚º2åˆ†é¡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # åˆ¤æ–·æ˜¯å¦ä½¿ç”¨GPUï¼Œå¦å‰‡ç”¨CPU
model = model.to(device)  # å°‡æ¨¡å‹æ¬ç§»è‡³æŒ‡å®šè£ç½®ï¼ˆGPUæˆ–CPUï¼‰

# è¼‰å…¥å·²æœ‰æ¨¡å‹æ¬Šé‡
model.load_state_dict(torch.load(model_path, map_location=device))
print(f"Model {model_path} weights loaded successfully.")
def predict_video(video_path, model=model, transform=None, original_frames=240, sample_frames=120):
    # é è¨­ä½¿ç”¨ SafeVideoAugmentationï¼Œé—œé–‰ blur/äº®åº¦èª¿æ•´ï¼ˆé©—è­‰æ™‚ä¹Ÿé€™æ¨£ï¼‰
    if transform is None:
        transform = SafeVideoAugmentation(apply_blur_prob=0.0, apply_brightness_prob=0.0)
    
    # è®€å½±ç‰‡ â†’ (C, T, H, W)
    video = read_video_cv2(video_path, original_frames, sample_frames)

    # (C, T, H, W) â†’ (T, H, W, C) â†’ numpy çµ¦ transform ç”¨
    video = video.permute(1, 2, 3, 0).numpy()
    
    # è³‡æ–™å¢å¼·ï¼ˆå…¶å¯¦åªæœ‰ resize + ç°éš + normalizeï¼‰
    video = transform(video)  # å›å‚³ (C, T, H, W)

    # åŠ  batch ç¶­åº¦ (1, C, T, H, W)
    video = video.unsqueeze(0).to(device)

    # æ¨è«–
    with torch.no_grad():
        outputs = model(video)
        probs = F.softmax(outputs, dim=1).cpu().numpy()[0]
        pred_label = np.argmax(probs)
        if pred_label == 0:
            pred_label = 'å£çƒ'
        if pred_label == 1:
            pred_label = 'å¥½çƒ'
    return pred_label